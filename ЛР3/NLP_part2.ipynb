{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1836d4d6-1154-4a4d-b216-c79feacef7ff",
   "metadata": {},
   "source": [
    "## NLP. Обработка естественного языка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0028f13c-7131-4de4-80ad-288648f2874e",
   "metadata": {},
   "source": [
    "В данной работе выполняется:\n",
    "1) обработка текста на основе правил\n",
    "2) интеграция языковой разметки spacy с моделью машинного обучения\n",
    "3) распознавание эмоциональной составляющей слов и предложений\n",
    "4) параллелизм подсчета слов с использованием MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b84c59-9b7d-47af-a5f2-06279fddadf9",
   "metadata": {},
   "source": [
    "#### 1. NER. Пользовательская разметка именованных сущностей на основе правил"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d9e654-f6a8-41d9-8fe7-da9a32d6d06e",
   "metadata": {},
   "source": [
    "Библиотека Python spaCy предлагает несколько различных методов для выполнения NER на основе правил. Одним из таких методов является EntityRuler.\n",
    "EntityRuler - это фабрика spaCy, которая позволяет создавать набор шаблонов с соответствующими метками. Фабрика в spaCy - это набор классов и функций, предварительно загруженных в spaCy, которые выполняют определенные задачи. В случае с EntityRuler фабрика позволяет пользователю создать EntityRuler, дать ему набор инструкций, а затем использовать эти инструкции для поиска и маркировки сущностей.\n",
    "\n",
    "После того как пользователь создал EntityRuler и задал ему набор инструкций, он может добавить его в конвейер spaCy как новую pipe.\n",
    "\n",
    "pipe - это компонент конвейера. Цель конвейера - принимать входные данные, выполнять над ними какие-то операции, а затем выводить эти операции либо в виде новых данных, либо в виде извлеченных метаданных. pipe - это отдельный компонент. В случае со spaCy есть несколько различных конвейеров, которые выполняют разные задачи. Токенизатор разбивает текст на отдельные лексемы, синтаксический анализатор разбирает текст, а NER идентифицирует сущности и присваивает им соответствующие метки.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deba11fd-22ac-4957-afa1-73d52744151a",
   "metadata": {},
   "source": [
    "Важно помнить, что конвейеры являются последовательными. Это означает, что компоненты, расположенные раньше в конвейере, влияют на то, что получают последующие компоненты. Иногда эта последовательность очень важна, то есть последующие компоненты зависят от предыдущих.\n",
    "\n",
    "В этом блокноте мы подробно рассмотрим EntityRuler как компонент конвейера модели spaCy. Готовые модели spaCy поставляются с предварительно загруженной моделью NER; однако они не поставляются с EntityRuler. Чтобы включить EntityRuler в модель spaCy, его нужно создать как новую трубу, дать инструкции, а затем добавить в модель. После этого пользователь может сохранить новую модель с EntityRuler на диск."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43127669-24ca-4cc7-bd54-81bcc97ba560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#импортируйте библиотеку spacy (см.лаб 2)\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93aba418-f636-406b-a0db-6bb5921acb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузите корпус русского языка в переменную nlp (см.лаб 2) \n",
    "\n",
    "nlp = spacy.load(\"ru_core_news_md\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b31e8e9a-31ad-4725-a72b-41471afbd8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#создайте EntityRuler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d18727a-d275-4375-9bf6-4e25e9c87768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#задайте сущности(категории) и значения\n",
    "#через следующий код: \n",
    "#patterns = [\n",
    "#                {\"label\": \"...\", \"pattern\": \"...\"}\n",
    "#            ]\n",
    "\n",
    "#где label - название сущности, pattern - значение\n",
    "# одна строка соответстивует одной сущности и одному значению, чтобы добавить следующую сущность и значение, воспользуйтесь запятой:\n",
    "#patterns = [\n",
    "#                {\"label\": \"...\", \"pattern\": \"...\"},\n",
    "#                {\"label\": \"...\", \"pattern\": \"...\"},\n",
    "#            ]\n",
    "# задайте сущность \"SEASON\" и значение \"зима\", задайте сущность \"PROFESSION\" и значение \"журналисты\", задайте сущность \"PROFESSION\" и значение \"исследователи\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fba8205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    {'label': 'SEASON', 'pattern': 'зима'},\n",
    "    {'label': 'PROFESSION', 'pattern': 'журналисты'},\n",
    "    {'label': 'PROFESSION', 'pattern': 'исследователи'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc61b151-c7a0-473d-a06c-e009556a40b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#добавьте описанные выше правила в обработчик nlp:\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcde8833-2138-4dce-835d-a893329f883f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "кажется, что сейчас мы переживаем «золотой век» искусственного интеллекта. компании инвестируют десятки миллиардов долларов в разработку инструментов на базе генеративного ии. журналисты и исследователи экспериментируют с chatgpt, прося его то сочинить стих, то написать статью. правительства многих стран обсуждают или уже принимают законы, регулирующие использование алгоритмов. однако в истории уже были похожие периоды эйфории по поводу искусственного интеллекта. и заканчивались они разочарованием, оттоком инвестиций и замедлением развития технологий. рассказываем, почему происходили эти спады и стоит ли ожидать схожий итог у нынешней ии-лихорадки.«зима искусственного интеллекта» — период, когда происходит снижение финансирования и интереса к исследованиям в области ии. термин впервые появился в 1984 году как тема публичной дискуссии на ежегодной встрече американской ассоциации искусственного интеллекта (aaai). на ней ведущие исследователи в области ии роджер шэнк и марвин мински высказали опасения насчет того, что энтузиазм вокруг ии выходит из-под контроля, и в перспективе ни к чему хорошему не приведет. они сравнили этот процесс с ядерной зимой — в том смысле, что будет запущена цепная реакция, когда ученые из-за неоправдавшихся ожиданий разочаруются в эффективности технологий, подобные пессимистические настроения начнут тиражировать сми, а инвесторы не захотят вкладываться в разработки с туманными перспективами. и в конце концов исследования окажутся полностью или почти полностью прекращены. неизвестно, как участники встречи отреагировали на предостережения коллег, но уже через три года их прогноз сбылся. впрочем, вернемся немного назад.первая зима ии продлилась с 1974 по 1980 год. до этого ей предшествовали практически два десятилетия активно нарастающего интереса к «мыслящим машинам» со стороны ученых и широкой общественности.в 1956 году в дартмутском колледже прошел двухмесячный семинар, на котором для обозначения новой области исследования, занимающейся моделированием человеческого разума, американский информатик джон маккарти предложил термин «искусственный интеллект». а за два года до этого в джорджтауне в штаб-квартире корпорации ibm был продемонстрирован полностью автоматический перевод более 60 предложений с русского языка на английский. презентация прошла успешно и, будучи широко разрекламированной в сми, побудила правительство сша начать больше инвестировать в компьютерную лингвистику. и не только в нее — с 1950-х по 1970-е годы управление перспективных исследовательских проектов министерства обороны сша (darpa) финансировало исследования в области искусственного интеллекта с минимальными требованиями к разработке проектов — фактически исследователи ии могли тратить полученные средства на любые проекты, которые им придут в голову. в этот период также появился предшественние современных нейросетей — перцептрон.но к 1970-м всеобщий энтузиазм начал затухать. во-первых, алгоритмы-предшественники онлайн-переводчика deepl хоть и подавали большие надежды, но без понимания контекста предложения выдавали некорректные результаты и в целом обходились дороже людей-переводчиков. во-вторых, в 1969 году вышла книга «перцептроны» марвина мински и сеймура пейперти, в которой ученые заявили, что хоть первые нейросетевые модели и показывали феноменальные способности к обучению, их функциональность для решения каких-либо задач на тот момент была крайне мала. в-третьих, повлияло и принятие поправки сенатора майкла мэнсфилда 1969 года. в соответствии с инициативой, darpa сокращала финансирование «общих исследований». теперь ученые были обязаны доказать, что их ии-исследования имеют «прямую и очевидную» пользу для конкретных военных задач. финальным аккордом стал отчет английского ученого джеймса лайтхилла, подготовленный в 1973 году. в нем английский математик дал крайне пессимистический прогноз насчет перспектив ии, отдельно отметив, что ни одно из открытий в этой области не оказало существенного влияния на науку и общество в целом.в результате накопившегося разочарования поток инвестиций иссяк, интерес правительств и широкой общественности к технологиям ии практически сошел на нет, что привело к остановке многих исследований и разработок."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#далее повторите шаги 1-4 из лаб 2:\n",
    "\n",
    "aiwinter = open('ЛР3/aiwinter.txt', encoding='utf8').read().replace('\\n', '').lower()\n",
    "doc = nlp(aiwinter)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c0d78db-ff6a-4b4f-b173-c5e197aad07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "журналисты PROFESSION\n",
      "исследователи PROFESSION\n",
      "chatgpt ORG\n",
      "ии LOC\n",
      "исследователи PROFESSION\n",
      "ии роджер шэнк PER\n",
      "марвин мински PER\n",
      "ии PER\n",
      "сми ORG\n",
      "зима SEASON\n",
      "общественности.в ORG\n",
      "джон маккарти PER\n",
      "джорджтауне LOC\n",
      "ibm ORG\n",
      "сми ORG\n",
      "сша LOC\n",
      "министерства обороны ORG\n",
      "сша (darpa) LOC\n",
      "исследователи PROFESSION\n",
      "ии PER\n",
      "deepl ORG\n",
      "марвина мински PER\n",
      "сеймура пейперти PER\n",
      "мала PER\n",
      "майкла мэнсфилда PER\n",
      "джеймса лайтхилла PER\n",
      "целом.в ORG\n"
     ]
    }
   ],
   "source": [
    "# найдите именованные сущности документа , выведите значение и сущность через ent.text, ent.label_ (см. в помощь лаб 2)\n",
    "\n",
    "for token in doc.ents:\n",
    "    print(token.text, token.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "645e6cfd-09f1-47a9-8292-bc689fd559ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавьте в patterns правило обнаружения года\n",
    "# для этого составьте правило (обновите значения переменной patterns), глядя по аналогии на пример описания корпусов кампусов формата 324-444 , 672-531:                 \n",
    "#                {\"label\": \"HOUSES\", \"pattern\": [{\"SHAPE\": \"ddd\"}, {\"ORTH\": \"-\"}, {\"SHAPE\": \"ddd\"}]}\n",
    "\n",
    "patterns.append({\"label\": \"HOUSES\", \"pattern\": [{\"SHAPE\": \"dddd\"}]})\n",
    "ruler.add_patterns(patterns)\n",
    "doc = nlp(aiwinter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "329e1317-9511-4ee5-9825-15544ea2b099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "журналисты PROFESSION\n",
      "исследователи PROFESSION\n",
      "chatgpt ORG\n",
      "ии LOC\n",
      "1984 HOUSES\n",
      "исследователи PROFESSION\n",
      "ии роджер шэнк PER\n",
      "марвин мински PER\n",
      "ии PER\n",
      "сми ORG\n",
      "зима SEASON\n",
      "1974 HOUSES\n",
      "1980 HOUSES\n",
      "общественности.в ORG\n",
      "1956 HOUSES\n",
      "джон маккарти PER\n",
      "джорджтауне LOC\n",
      "ibm ORG\n",
      "сми ORG\n",
      "сша LOC\n",
      "министерства обороны ORG\n",
      "сша (darpa) LOC\n",
      "исследователи PROFESSION\n",
      "ии PER\n",
      "deepl ORG\n",
      "1969 HOUSES\n",
      "марвина мински PER\n",
      "сеймура пейперти PER\n",
      "мала PER\n",
      "майкла мэнсфилда PER\n",
      "1969 HOUSES\n",
      "джеймса лайтхилла PER\n",
      "1973 HOUSES\n",
      "целом.в ORG\n"
     ]
    }
   ],
   "source": [
    "# добавьте описанные выше правила в обработчик nlp, примените языковую модель (шаг 4 из лаб 2)\n",
    "# найдите именованные сущности документа, должны распознаться года\n",
    "\n",
    "for token in doc.ents:\n",
    "    print(token.text, token.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f73a21-3fea-473c-9e7f-b982a2b71689",
   "metadata": {},
   "source": [
    "#### 2. Sentiment Analysis Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd7f0bd-fae3-42c2-8e46-4cc3e75ae97f",
   "metadata": {},
   "source": [
    "Это процесс анализа текста с целью определения, выражает ли он положительное, отрицательное или нейтральное настроение."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca8efd9-a9ec-4eb9-bdcd-8b57d9369a8f",
   "metadata": {},
   "source": [
    "Один из самых простых и старых подходов к анализу настроений — использовать набор предопределенных правил (подобно тому, как мы описывали правила для NER выше) и лексиконов для назначения баллов полярности словам или фразам. Например, модель на основе правил может назначать положительный балл таким словам, как «любовь», «счастливый» или «удивительный», и отрицательный балл таким словам, как «ненависть», «грустный» или «ужасный». Затем модель будет объединять баллы слов в тексте, чтобы определить его общую тональность. Модели на основе правил легко реализовать и интерпретировать, но у них есть несколько серьезных недостатков. Они не способны улавливать контекст, сарказм или нюансы языка, и они требуют много ручных усилий для создания и поддержания правил и лексиконов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a4fae4-ecbb-4bdd-8221-67001a6504ed",
   "metadata": {},
   "source": [
    "Другой подход к анализу настроений заключается в использовании моделей машинного обучения, которые представляют собой алгоритмы, обучающиеся на данных и делающие прогнозы на основе шаблонов и признаков. Модели машинного обучения могут быть как с учителем, так и без учителя, в зависимости от того, используют ли они маркированные или немаркированные данные для обучения. Модели машинного обучения с учителем, такие как логистическая регрессия, метод опорных векторов или нейронные сети, учатся классифицировать тексты по предопределенным категориям, таким как положительные, отрицательные или нейтральные, на основе маркированных примеров. Модели машинного обучения без учителя, такие как кластеризация, тематическое моделирование или встраивание слов, учатся обнаруживать скрытую структуру и значение текстов на основе немаркированных данных. Модели машинного обучения более гибкие и мощные, чем модели на основе правил, но у них также есть некоторые проблемы. Они требуют большого объема данных и вычислительных ресурсов, они могут быть предвзятыми или неточными из-за качества данных или выбора признаков, и их может быть трудно объяснить или понять."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980431bb-aef6-45ea-b8fc-ded5ea1a3a04",
   "metadata": {},
   "source": [
    "Прежде, чем выполнять вычисления, необходимо слова представить в векторной форме. Цель вектора слов — заставить вычислительную систему понять слово. Вычислительные системы не могут эффективно понимать текст. Однако они могут быстро и эффективо обрабатывать числа. По этой причине важно преобразовать слово в число."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f1038a-b711-4667-a7de-a9a12ebcb0e3",
   "metadata": {},
   "source": [
    "В данном задании вам необходимо создать модель ML и обучить её распознавать тональность слова, предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "110c07e0-507a-46fa-9e15-466f2f358377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируйте pandas\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9105d55f-f801-41a3-9d89-bba2e03f6c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>не оказало</td>\n",
       "      <td>негатив</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        topic    label\n",
       "0  не оказало  негатив"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загрузите в dataframe файл sentiment.csv, разделитель \";\" , encoding = 'utf8'\n",
    "# дадайте имена столбцам: 'topic', 'label'\n",
    "\n",
    "df = pd.read_csv('ЛР3/sentiment.csv',\n",
    "                 sep=';',\n",
    "                 encoding='utf8',\n",
    "                 names=['topic', 'label'])\n",
    "df.iloc[0:1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be580382-bddd-45fc-b406-e3bb1cda3a3e",
   "metadata": {},
   "source": [
    "Файл содержит разметку слов на положительные и отрицательные слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b5f973c-c9e9-48a6-96a9-ea0bda0df54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируйте из библиотеки sklearn модуль preprocessing \n",
    "# подключите функцию LabelEncoder() \n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efacb57-198d-4209-8c07-f21356e56d7f",
   "metadata": {},
   "source": [
    "Label Encoding - это техника, которая используется для преобразования категориальных столбцов в числовые, чтобы их можно было использовать в моделях машинного обучения, которые принимают только числовые данные. Это важный этап предварительной обработки в проекте машинного обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4fdf8d5f-0bf7-4ee0-b22d-e874956104a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>label</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>не оказало</td>\n",
       "      <td>негатив</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        topic    label  label_num\n",
       "0  не оказало  негатив          0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создайте дополнительный столбец 'label_num' и запишите туда данные столбца 'label', к которым был применен LabelEncoder() \n",
    "# таким образом будут столбцы: 'topic', 'label', 'label_num'\n",
    "# le.fit(df['label'])\n",
    "df['label_num'] = le.fit_transform(df['label'])\n",
    "df.iloc[0:1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f9bbaae-6f4b-4073-a618-e0b60756221d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>label</th>\n",
       "      <th>label_num</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>не оказало</td>\n",
       "      <td>негатив</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.08277767, 0.024399191, 0.28569317, 0.305388...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        topic    label  label_num  \\\n",
       "0  не оказало  негатив          0   \n",
       "\n",
       "                                              vector  \n",
       "0  [0.08277767, 0.024399191, 0.28569317, 0.305388...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# трансформируйте слова столбца 'topic' в векторную форму и запишите результат в новый столбец 'vector'\n",
    "# для трансформации в векторную форму пользуйтесь имеющимся конвейером nlp от spacy, пример вызова : nlp(text).vector\n",
    "# подсказка: простым способом трансформации будет использование lambda - функции к столбцу 'topic'\n",
    "\n",
    "df['vector'] = df['topic'].apply(lambda x: nlp(x).vector)\n",
    "df.iloc[0:1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0812825d-7a69-4941-bd06-ec54fa996a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#импортируйте train_test_split из sklearn.model_selection \n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e965303-9da0-462e-80a3-259be67930c7",
   "metadata": {},
   "source": [
    "Разделите на обучающие и тестовые выборки :\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "где \n",
    "\n",
    "X - столбец значений векторов вашего датафрейма, т.е. имя_датафрейма.vector.values,\n",
    "\n",
    "Y - столбец label_num вашего датафрейма\n",
    "\n",
    "test_size - 20%, то есть 80% датафрейма берется на обучение, 20% на тестирование\n",
    "\n",
    "random_state установите 2022\n",
    "\n",
    "https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b208f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['vector'].values,\n",
    "                                                    df['label_num'].values,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=2022)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b16e251-2837-46cd-a454-7537d53ba550",
   "metadata": {},
   "source": [
    " #### 2.1 Naive Bayes sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a12da4-f1e1-49a5-b058-db75d63338f7",
   "metadata": {},
   "source": [
    "Алгоритм Naive Bayes - это метод классификации, основанный на теореме Байеса. Он используется для предсказания класса наблюдения по набору признаков. Алгоритм Naive Bayes считается одним из самых простых и мощных алгоритмов машинного обучения.\n",
    "\n",
    "Алгоритм Naive Bayes основан на концепции условной вероятности. В нем делается предположение, что наличие определенного признака в классе не связано с наличием других признаков в этом классе. Это предположение называется условной независимостью класса."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693e5ce1-ff63-446d-b8c3-8f49310fec95",
   "metadata": {},
   "source": [
    "Алгоритм Naive Bayes можно применить к любой задаче классификации, когда у нас есть набор признаков и мы хотим предсказать метку класса. Некоторые распространенные примеры:\n",
    "\n",
    "Фильтрация спама: У нас есть набор электронных писем, каждое из которых представлено набором слов (признаков), и мы хотим классифицировать их как спам или не спам.\n",
    "\n",
    "Классификация документов: У нас есть набор документов, каждый из которых представлен набором слов (признаков), и мы хотим классифицировать их по различным категориям (например, спорт, политика, технологии и т. д.).\n",
    "\n",
    "Анализ настроений: У нас есть набор рецензий на фильмы, каждая из которых представлена набором слов (признаков), и мы хотим классифицировать их как положительные или отрицательные.\n",
    "\n",
    "Идентификация автора: Имеется набор письменных текстов, каждый из которых представлен словом (признаками), и задача состоит в том, чтобы определить вероятного автора из множества потенциальных авторов. Для этого необходимо проанализировать стиль письма и языковые особенности, чтобы приписать конкретные тексты их авторам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eeafc8db-49e0-4d2a-b5a4-555d9cef9465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#необходимо конвертиовать в двумерный массив перед обучением модели, так как сейчас в X_train каждый элемент был отдельным numpy массивом\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X_train_2d = np.stack(X_train)\n",
    "X_test_2d = np.stack(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d6f5f1-bdb5-4c8d-af55-7c32e38a2bf0",
   "metadata": {},
   "source": [
    "Алгоритм классификации Multinomial Naive Bayes, как правило, является базовым решением для задачи анализа настроений. Основная идея метода Naive Bayes заключается в том, чтобы найти вероятности классов, присвоенных текстам, используя совместные вероятности слов и классов.\n",
    "\n",
    "Мультиномиальный NB - это еще один вариант классификатора NB, который часто используется в задачах классификации текстов, включая SA. Он особенно подходит для признаков, представляющих собой частоты или количества слов, которые обычно получают с помощью таких методов, как модель мешка слов или TF-IDF.Мультиномиальный NB-классификатор предполагает, что признаки условно независимы друг от друга, учитывая метку класса, и следуют мультиномиальному распределению. Он рассчитывает вероятность принадлежности документа к определенному классу на основе частот или количества признаков (слов) в документе."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52276649-3953-4b12-8e92-52c931c7e5e6",
   "metadata": {},
   "source": [
    "Так как массив содержит отрицательные значения, модель не обучится. Трансформируем в положительные начения, используя:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67e6d23d-f4ff-4acc-9afa-730c39c38b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_X_train_2d = scaler.fit_transform(X_train_2d)\n",
    "scaled_X_test_2d = scaler.transform(X_test_2d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc2ddb24-43e3-456e-b415-8c20c0cc84b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#импортируйте MultinomialNB из sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd5e83cc-d9d8-4265-8c7d-868b0d2e8bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создайте мультиномиальный классификатор Naive Bayes, т.е. создайте экземпляр модели MultinomialNB() в переменную\n",
    "MNB = MultinomialNB()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd0cbae-42a2-42e0-97ad-dc4ab83e5095",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/dev/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b9dac2c-3ff4-447f-9d52-4d381cb4d719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MultinomialNB<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">?<span>Documentation for MultinomialNB</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MultinomialNB()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# обучите классификатор с пом. функции fit, на вход функции передайте scaled_X_train_2d и y_train\n",
    "MNB.fit(scaled_X_train_2d, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c0d97e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = MNB.predict(scaled_X_test_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b258d2f3-ef88-4567-9f9f-51b6ef7c4821",
   "metadata": {},
   "source": [
    "Вызовите функцию https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.classification_report.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a549e69-e9ee-4fd8-be04-5e06ff285379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         3\n",
      "           1       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.88      0.75      0.76         5\n",
      "weighted avg       0.85      0.80      0.78         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# передайте в функцию y_test, y_pred\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9f6fc5-26dc-40bc-b795-c9b194be525f",
   "metadata": {},
   "source": [
    "Модель обучена. протестируйте её с помощью функции predict и пеердачи на вход этой функции scaled_test_embed\n",
    "\n",
    "https://scikit-learn.org/dev/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5946241e-0d41-4841-9dad-4cf974ceb302",
   "metadata": {},
   "source": [
    "Определите тональность слов с помощью обученной модели:\n",
    "\n",
    "1) пессимист\n",
    "2) разочаровал\n",
    "3) выберите несколько предложений из файла"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59955bf7-dc8b-4445-bcbb-e41bb34c6a84",
   "metadata": {},
   "source": [
    "!!! не забудьте :\n",
    "1) трансформировать слова/предложения в вектор\n",
    "2) применить np.stack\n",
    "3) при выполнении функции выполнить .reshape(1, -1) к значению после п.2\n",
    "4) вызвать функцию .predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4e29f044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[0]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "my_test1 = np.stack(nlp('пессимист').vector).reshape(1, -1)\n",
    "my_test2 = np.stack(nlp('разочаровал').vector).reshape(1, -1)\n",
    "my_test3 = np.stack(nlp('Впрочем, вернемся немного назад.Первая зима ИИ продлилась с 1974 по 1980 год.'.replace('\\n', '').lower()).vector).reshape(1, -1)\n",
    "my_test4 = np.stack(nlp('В нем английский математик дал крайне пессимистический прогноз насчет перспектив ИИ, отдельно отметив, что ни одно из открытий в этой области не оказало существенного влияния на науку и общество в целом.'.replace('\\n', '').lower()).vector).reshape(1, -1)\n",
    "\n",
    "my_test5 = np.stack(nlp('потенциалом').vector).reshape(1, -1)\n",
    "# 3) выберите несколько предложений из файла\n",
    "\n",
    "\n",
    "print(MNB.predict(my_test1))\n",
    "print(MNB.predict(my_test2))\n",
    "print(MNB.predict(my_test3))\n",
    "print(MNB.predict(my_test4))\n",
    "print(MNB.predict(my_test5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e351f580-bc53-499d-8226-97c178be73b2",
   "metadata": {},
   "source": [
    " #### 2.2 Обучите модель распознавать тональность"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b3b32f-05b2-4393-b638-b8332f81e35c",
   "metadata": {},
   "source": [
    "1) самостоятельно выберите предложения для обучения\n",
    "2) сделайте разметку предложений на позитивные, негативные, нейтральные\n",
    "3) обучите модель распознавать тональность на основе MultinominalNB (выполните те же шаги, что и выше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7605aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sent.text for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94d55fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>они сравнили этот процесс с ядерной зимой — в ...</td>\n",
       "      <td>негатив</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>и в конце концов исследования окажутся полност...</td>\n",
       "      <td>негатив</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>кажется, что сейчас мы переживаем «золотой век...</td>\n",
       "      <td>позитив</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               topic    label\n",
       "0  они сравнили этот процесс с ядерной зимой — в ...  негатив\n",
       "1  и в конце концов исследования окажутся полност...  негатив\n",
       "2  кажется, что сейчас мы переживаем «золотой век...  позитив"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sent = pd.read_csv('ЛР3/my_sentiment.csv', encoding='utf8', sep=';', names=['topic', 'label'])\n",
    "df_sent.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ec6aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "le_sent = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7fa391aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>label</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>они сравнили этот процесс с ядерной зимой — в ...</td>\n",
       "      <td>негатив</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>и в конце концов исследования окажутся полност...</td>\n",
       "      <td>негатив</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>кажется, что сейчас мы переживаем «золотой век...</td>\n",
       "      <td>позитив</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>компании инвестируют десятки миллиардов доллар...</td>\n",
       "      <td>позитив</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>журналисты и исследователи экспериментируют с ...</td>\n",
       "      <td>позитив</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               topic    label  label_num\n",
       "0  они сравнили этот процесс с ядерной зимой — в ...  негатив          0\n",
       "1  и в конце концов исследования окажутся полност...  негатив          0\n",
       "2  кажется, что сейчас мы переживаем «золотой век...  позитив          2\n",
       "3  компании инвестируют десятки миллиардов доллар...  позитив          2\n",
       "4  журналисты и исследователи экспериментируют с ...  позитив          2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sent['label_num'] = le_sent.fit_transform(df_sent['label'])\n",
    "df_sent.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f53c220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>label</th>\n",
       "      <th>label_num</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>они сравнили этот процесс с ядерной зимой — в ...</td>\n",
       "      <td>негатив</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.026646322, -0.09245006, 0.06024115, -0.0310...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               topic    label  label_num  \\\n",
       "0  они сравнили этот процесс с ядерной зимой — в ...  негатив          0   \n",
       "\n",
       "                                              vector  \n",
       "0  [0.026646322, -0.09245006, 0.06024115, -0.0310...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sent['vector'] = df_sent['topic'].apply(lambda x: nlp(x).vector)\n",
    "df_sent.iloc[0:1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "79eadeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.60      0.50         5\n",
      "           1       0.33      0.50      0.40         2\n",
      "           2       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.40        10\n",
      "   macro avg       0.25      0.37      0.30        10\n",
      "weighted avg       0.28      0.40      0.33        10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/ds/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/anaconda3/envs/ds/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/anaconda3/envs/ds/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, Y_train, Y_test = train_test_split(df_sent['vector'].values,\n",
    "                                                    df_sent['label_num'].values,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=None\n",
    "                                                    )\n",
    "\n",
    "\n",
    "x_train_2d = np.stack(x_train)\n",
    "x_test_2d = np.stack(x_test)\n",
    "\n",
    "\n",
    "scaler_sent = MinMaxScaler()\n",
    "scaled_x_train_2d = scaler_sent.fit_transform(x_train_2d)\n",
    "scaled_x_test_2d = scaler_sent.transform(x_test_2d)\n",
    "\n",
    "\n",
    "MNB_sent = MultinomialNB()\n",
    "MNB_sent.fit(scaled_x_train_2d, Y_train)\n",
    "Y_pred = MNB.predict(scaled_x_test_2d)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75326bb0-33f6-45f9-90dd-ab578319adcd",
   "metadata": {},
   "source": [
    "#### 3. MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90384817-b275-4eda-8540-b18a6b8aea96",
   "metadata": {},
   "source": [
    "От современных приложений добычи данных, которые часто называют анализом \"больших данных\", требуется способность быстро обрабатывать огромные массивы данных. Во многих таких приложениях структура данных регулярна и существует масса возможностей распараллеливания. \n",
    "\n",
    "Одна из возможной - технология MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456eeb6c-d7a4-461d-86e8-b921b5c498e1",
   "metadata": {},
   "source": [
    "Процесс выглядит следующим образом:\n",
    "1) есть несколько задач Map(распределителей), каждая из которых получает одно или несколько порций файла из распределенной файловой системы. Распределители преобразуют порцию в последовательность пар ключ-значение. Как именно из входных данных порождаются эти пары, определяет функция Map, написанная пользователем. Такие распредлители находятся на разных вычислительных узлах. Один Map на один вычислительный узел/выч.машину.\n",
    "2) Пары ключ-значение от каждого распределителя собираются главным контроллером и сортируются по ключу. Затем ключи раздаются задачам Reduce(редукторам), так что все пары с одинаковым ключом попадают одному и тому же редуктору.\n",
    "3) Редукторы обрабатывают по одному ключу за раз и таким образом комбинируют значения, ассоциированые с этим ключом. Способ комбинирования определяется функцией Reduce, написанной пользователем."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a74e1a7-7f74-4626-8aa8-b7abd6acb0e9",
   "metadata": {},
   "source": [
    " #### 3.1 Реализация функции Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16339251-0d66-48f3-9b6f-5ff5431706e6",
   "metadata": {},
   "source": [
    "В этой части мы создадим набор пар ключ-значение и соберем все пары с одинаковым ключом.\n",
    "\n",
    "В качестве исходных данных здесь используется файл «aiwinter.txt», который мы обрабатывали в лаб.2. Вам потребуется в этом блокноте выполнить шаги 1 -3 лаб 2.\n",
    "\n",
    "Далее выполнить (ранее это также было выполнено на шаге 11 в лаб.2) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0de1b24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "кажется, что сейчас мы переживаем «золотой век» искусственного интеллекта. компании инвестируют десятки миллиардов долларов в разработку инструментов на базе генеративного ии. журналисты и исследователи экспериментируют с chatgpt, прося его то сочинить стих, то написать статью. правительства многих стран обсуждают или уже принимают законы, регулирующие использование алгоритмов. однако в истории уже были похожие периоды эйфории по поводу искусственного интеллекта. и заканчивались они разочарованием, оттоком инвестиций и замедлением развития технологий. рассказываем, почему происходили эти спады и стоит ли ожидать схожий итог у нынешней ии-лихорадки.«зима искусственного интеллекта» — период, когда происходит снижение финансирования и интереса к исследованиям в области ии. термин впервые появился в 1984 году как тема публичной дискуссии на ежегодной встрече американской ассоциации искусственного интеллекта (aaai). на ней ведущие исследователи в области ии роджер шэнк и марвин мински высказали опасения насчет того, что энтузиазм вокруг ии выходит из-под контроля, и в перспективе ни к чему хорошему не приведет. они сравнили этот процесс с ядерной зимой — в том смысле, что будет запущена цепная реакция, когда ученые из-за неоправдавшихся ожиданий разочаруются в эффективности технологий, подобные пессимистические настроения начнут тиражировать сми, а инвесторы не захотят вкладываться в разработки с туманными перспективами. и в конце концов исследования окажутся полностью или почти полностью прекращены. неизвестно, как участники встречи отреагировали на предостережения коллег, но уже через три года их прогноз сбылся. впрочем, вернемся немного назад.первая зима ии продлилась с 1974 по 1980 год. до этого ей предшествовали практически два десятилетия активно нарастающего интереса к «мыслящим машинам» со стороны ученых и широкой общественности.в 1956 году в дартмутском колледже прошел двухмесячный семинар, на котором для обозначения новой области исследования, занимающейся моделированием человеческого разума, американский информатик джон маккарти предложил термин «искусственный интеллект». а за два года до этого в джорджтауне в штаб-квартире корпорации ibm был продемонстрирован полностью автоматический перевод более 60 предложений с русского языка на английский. презентация прошла успешно и, будучи широко разрекламированной в сми, побудила правительство сша начать больше инвестировать в компьютерную лингвистику. и не только в нее — с 1950-х по 1970-е годы управление перспективных исследовательских проектов министерства обороны сша (darpa) финансировало исследования в области искусственного интеллекта с минимальными требованиями к разработке проектов — фактически исследователи ии могли тратить полученные средства на любые проекты, которые им придут в голову. в этот период также появился предшественние современных нейросетей — перцептрон.но к 1970-м всеобщий энтузиазм начал затухать. во-первых, алгоритмы-предшественники онлайн-переводчика deepl хоть и подавали большие надежды, но без понимания контекста предложения выдавали некорректные результаты и в целом обходились дороже людей-переводчиков. во-вторых, в 1969 году вышла книга «перцептроны» марвина мински и сеймура пейперти, в которой ученые заявили, что хоть первые нейросетевые модели и показывали феноменальные способности к обучению, их функциональность для решения каких-либо задач на тот момент была крайне мала. в-третьих, повлияло и принятие поправки сенатора майкла мэнсфилда 1969 года. в соответствии с инициативой, darpa сокращала финансирование «общих исследований». теперь ученые были обязаны доказать, что их ии-исследования имеют «прямую и очевидную» пользу для конкретных военных задач. финальным аккордом стал отчет английского ученого джеймса лайтхилла, подготовленный в 1973 году. в нем английский математик дал крайне пессимистический прогноз насчет перспектив ии, отдельно отметив, что ни одно из открытий в этой области не оказало существенного влияния на науку и общество в целом.в результате накопившегося разочарования поток инвестиций иссяк, интерес правительств и широкой общественности к технологиям ии практически сошел на нет, что привело к остановке многих исследований и разработок."
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(aiwinter)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b695f79-058e-48b6-ae96-2dde354dd1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = [ token.text for token in doc if token.pos_ == 'NOUN' and token.is_punct !=True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b17a7a2-15fc-4352-be78-36f4914c8475",
   "metadata": {},
   "source": [
    "Создайте пары ключ-значение, под ключом имеется в виду слово из nouns, под значением - идентификатор. Сейчас всем словам назначается идентификатор 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a33854d6-febc-46bd-8b80-a8d50838d996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['век 1', 'интеллекта 1']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#создайте пустой список\n",
    "lst = []\n",
    "\n",
    "\n",
    "\n",
    "# создайте цикл. для каждого значения* из nouns сделать: новая переменная присвоить ей значение* + пробел + 1 под типом данных str\n",
    "for noun in nouns:\n",
    "    lst.append(f'{noun} 1')\n",
    "# добавьте в список новая переменная\n",
    "\n",
    "lst[0:2]\n",
    "# результат должен быть таким ['c 1', 'a 1', 's 1', ...]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a08c06-227e-4bdd-8009-e806eb8712fb",
   "metadata": {},
   "source": [
    "Выполните сортировку. Теперь мы сгруппируем вложенные структуры с одинаковыми ключами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8cde39ae-a099-493b-b1ab-72fb10341b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#примените к вашему списку функцию .sort()\n",
    "# таким образом, результат буде ['a 1', 'c 1', 's 1', ...]\n",
    "lst.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b5c17b-aee1-46ef-b1a3-97f2110787a2",
   "metadata": {},
   "source": [
    " #### 3.2 Реализация функции Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac90f825-abcf-4167-8f6a-5f08500a550c",
   "metadata": {},
   "source": [
    "В этой части мы соберем все значения с одинаковым ключом, подсчитаем их количество и выдадим результат."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa4012a-75ea-49e1-bef2-8991cd8ef5ab",
   "metadata": {},
   "source": [
    "выполните dict.fromkeys(ваш список)\n",
    "\n",
    "и примените к выражению выше конвертацию в list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9f6c6750-609e-4d6e-9847-7ce34c54079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = dict.fromkeys(lst, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d25ed55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in lst:\n",
    "    dct[key] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e3bca819",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sorted(dct.items(), key=lambda x: (-x[1], x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8f691320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('интеллекта 1', 5),\n",
       " ('области 1', 5),\n",
       " ('году 1', 4),\n",
       " ('исследования 1', 4),\n",
       " ('года 1', 3),\n",
       " ('исследователи 1', 3),\n",
       " ('ученые 1', 3),\n",
       " ('задач 1', 2),\n",
       " ('ии 1', 2),\n",
       " ('инвестиций 1', 2),\n",
       " ('интереса 1', 2),\n",
       " ('исследований 1', 2),\n",
       " ('период 1', 2),\n",
       " ('прогноз 1', 2),\n",
       " ('проектов 1', 2),\n",
       " ('сми 1', 2),\n",
       " ('термин 1', 2),\n",
       " ('технологий 1', 2),\n",
       " ('энтузиазм 1', 2),\n",
       " ('аккордом 1', 1),\n",
       " ('алгоритмов 1', 1),\n",
       " ('алгоритмы 1', 1),\n",
       " ('ассоциации 1', 1),\n",
       " ('базе 1', 1),\n",
       " ('век 1', 1),\n",
       " ('влияния 1', 1),\n",
       " ('встрече 1', 1),\n",
       " ('встречи 1', 1),\n",
       " ('год 1', 1),\n",
       " ('годы 1', 1),\n",
       " ('голову 1', 1),\n",
       " ('десятилетия 1', 1),\n",
       " ('десятки 1', 1),\n",
       " ('джорджтауне 1', 1),\n",
       " ('дискуссии 1', 1),\n",
       " ('долларов 1', 1),\n",
       " ('журналисты 1', 1),\n",
       " ('законы 1', 1),\n",
       " ('замедлением 1', 1),\n",
       " ('зима 1', 1),\n",
       " ('зимой 1', 1),\n",
       " ('инвесторы 1', 1),\n",
       " ('инициативой 1', 1),\n",
       " ('инструментов 1', 1),\n",
       " ('интеллект 1', 1),\n",
       " ('интерес 1', 1),\n",
       " ('информатик 1', 1),\n",
       " ('использование 1', 1),\n",
       " ('исследованиям 1', 1),\n",
       " ('иссяк 1', 1),\n",
       " ('истории 1', 1),\n",
       " ('итог 1', 1),\n",
       " ('квартире 1', 1),\n",
       " ('книга 1', 1),\n",
       " ('коллег 1', 1),\n",
       " ('колледже 1', 1),\n",
       " ('компании 1', 1),\n",
       " ('контекста 1', 1),\n",
       " ('контроля 1', 1),\n",
       " ('конце 1', 1),\n",
       " ('концов 1', 1),\n",
       " ('корпорации 1', 1),\n",
       " ('лингвистику 1', 1),\n",
       " ('лихорадки 1', 1),\n",
       " ('людей 1', 1),\n",
       " ('математик 1', 1),\n",
       " ('машинам 1', 1),\n",
       " ('миллиардов 1', 1),\n",
       " ('министерства 1', 1),\n",
       " ('модели 1', 1),\n",
       " ('моделированием 1', 1),\n",
       " ('момент 1', 1),\n",
       " ('надежды 1', 1),\n",
       " ('настроения 1', 1),\n",
       " ('науку 1', 1),\n",
       " ('нейросетей 1', 1),\n",
       " ('обозначения 1', 1),\n",
       " ('обороны 1', 1),\n",
       " ('обучению 1', 1),\n",
       " ('общественности 1', 1),\n",
       " ('общество 1', 1),\n",
       " ('ожиданий 1', 1),\n",
       " ('онлайн 1', 1),\n",
       " ('опасения 1', 1),\n",
       " ('остановке 1', 1),\n",
       " ('открытий 1', 1),\n",
       " ('оттоком 1', 1),\n",
       " ('отчет 1', 1),\n",
       " ('перевод 1', 1),\n",
       " ('переводчика 1', 1),\n",
       " ('переводчиков 1', 1),\n",
       " ('периоды 1', 1),\n",
       " ('перспектив 1', 1),\n",
       " ('перспективами 1', 1),\n",
       " ('перспективе 1', 1),\n",
       " ('перцептроны 1', 1),\n",
       " ('поводу 1', 1),\n",
       " ('пользу 1', 1),\n",
       " ('понимания 1', 1),\n",
       " ('поправки 1', 1),\n",
       " ('поток 1', 1),\n",
       " ('правительств 1', 1),\n",
       " ('правительства 1', 1),\n",
       " ('правительство 1', 1),\n",
       " ('предложений 1', 1),\n",
       " ('предложения 1', 1),\n",
       " ('предостережения 1', 1),\n",
       " ('предшественние 1', 1),\n",
       " ('предшественники 1', 1),\n",
       " ('презентация 1', 1),\n",
       " ('принятие 1', 1),\n",
       " ('проекты 1', 1),\n",
       " ('процесс 1', 1),\n",
       " ('развития 1', 1),\n",
       " ('разочарованием 1', 1),\n",
       " ('разочарования 1', 1),\n",
       " ('разработке 1', 1),\n",
       " ('разработки 1', 1),\n",
       " ('разработку 1', 1),\n",
       " ('разработок 1', 1),\n",
       " ('разума 1', 1),\n",
       " ('реакция 1', 1),\n",
       " ('результате 1', 1),\n",
       " ('результаты 1', 1),\n",
       " ('решения 1', 1),\n",
       " ('семинар 1', 1),\n",
       " ('сенатора 1', 1),\n",
       " ('смысле 1', 1),\n",
       " ('снижение 1', 1),\n",
       " ('соответствии 1', 1),\n",
       " ('спады 1', 1),\n",
       " ('способности 1', 1),\n",
       " ('средства 1', 1),\n",
       " ('статью 1', 1),\n",
       " ('стих 1', 1),\n",
       " ('стороны 1', 1),\n",
       " ('стран 1', 1),\n",
       " ('тема 1', 1),\n",
       " ('технологиям 1', 1),\n",
       " ('требованиями 1', 1),\n",
       " ('управление 1', 1),\n",
       " ('участники 1', 1),\n",
       " ('ученого 1', 1),\n",
       " ('ученых 1', 1),\n",
       " ('финансирование 1', 1),\n",
       " ('финансирования 1', 1),\n",
       " ('функциональность 1', 1),\n",
       " ('целом 1', 1),\n",
       " ('штаб 1', 1),\n",
       " ('эйфории 1', 1),\n",
       " ('эффективности 1', 1),\n",
       " ('языка 1', 1)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
